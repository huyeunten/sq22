---
title: "Homelessness"
author: "Haley Uyeunten"
output: html_document
editor_options: 
chunk_output_type: inline
---
## Introduction

The 2020 [point-in-time count](https://www.kingcounty.gov/elected/executive/constantine/news/release/2020/July/01-homeless-count.aspx) of people experiencing homelessness for Seattle/King County was 11,751. This represents a 5% increase over the 2019 count and reflects similar trend across many counties in the western U.S. A step towards addressing homelessness is improving our understanding of the relationship between local housing market factors and homelessness. 

The U.S. Department of Housing and Urban Development (HUD) produced a report in 2019 [Market Predictors of Homelessness](https://www.huduser.gov/portal/sites/default/files/pdf/Market-Predictors-of-Homelessness.pdf) that describes a model-based approach to understanding of the relationship between local housing market factors, policies, demographics, climate and homelessness. Our project is motivated by the goals of the HUD study:

"To continue progressing toward the goals of ending and preventing homelessness, we must further our knowledge of the basic community-level determinants of homelessness. The primary objectives of this study are to (1) identify market factors that have established effects on homelessness, (2) construct and evaluate empirical models of community-level homelessness.."

We will investigate whether there are alternative modeling approaches that outperform the models described in the HUD report.

We want to specifically look at which HUD variables are best at predicting homeless rates. The HUD report includes predictors that have to do with the housing market, economy, local safety nets, and climate, as well as demographic and identifier variables.

## Data Collection

The data for this project are described in HUD's report [Market Predictors of Homelessness](https://www.huduser.gov/portal/sites/default/files/pdf/Market-Predictors-of-Homelessness.pdf) in the section titled DATA.

I will refer you to this section of the HUD report for a detailed description of the sources of the data and how they were processed.


### Load necessary packages

```{r}

#skimr provides a nice summary of a data set
library(skimr)
#leaps will be used for model selection
library(leaps)
#readxl lets us read Excel files
library(readxl)
#GGally has a nice pairs plotting function
library(GGally)
#corrplot has nice plots for correlation matrices
library(corrplot)
#gridExtra
library(gridExtra)
#glmnet is used to fit glm's. It will be used for lasso and ridge regression models.
library(glmnet)
#tidymodels has a nice workflow for many models. We will use it for XGBoost
library(tidymodels)
#xgboost lets us fit XGBoost models
library(xgboost)
#vip is used to visualize the importance of predicts in XGBoost models
library(vip)
#tidyverse contains packages we will use for processing and plotting data
library(tidyverse)

#Set the plotting theme
theme_set(theme_bw())

```


### Examine the data dictionary

The data dictionary `HUD TO3 - 05b Analysis File - Data Dictionary.xlsx` contains descriptions of all variables in the data set.

$\rightarrow$ Load the data dictionary (call it `dictionary`) and view its contents using the function `View`.

```{r}
dictionary <- read_xlsx("HUD TO3 - 05b Analysis File - Data Dictionary.xlsx")
view(dictionary)
```


#### What are the sources of data?

$\rightarrow$ Use the dictionary to find the unique sources of data that are not derived from other variables. We will assume that derived variables are either have `Derived` equal to `No` or `Source or Root Variable` starts with the string `See`.

<details>
  <summary>**Show Coding Hint**</summary>

You can use the `str_detect` function to determine whether a string includes particular string as a component.

</details>  

```{r}
dictionary %>% 
  filter(Derived == "No" & str_detect(`Source or Root Variable`, "See") == FALSE) %>%  
  select(`Source or Root Variable`) %>% 
  unique()
```


What are the sources of most of the data?

$\rightarrow$ Make a bar graph of the counts of different data sources described in `Source or Root Variable`. Your graph should have the following features:

1. Order the bars in descending order based on the count.
2. Only include the 10 most common data sources.
3. Orient the plot so that it is easy to read the labels.

```{r}
dictionary %>% 
  count(`Source or Root Variable`) %>% 
  mutate(`Source or Root Variable` = fct_reorder(`Source or Root Variable`, n)) %>% 
  head(10) %>% 
  ggplot(aes(x = `Source or Root Variable`, y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Source or Root Variable", y = "Count")
```
Most of the data comes from the census ACS 5-year estimates. Some of the data also comes from local area unemployment statistics, census building permits survey, and the census. Many of the variables come from various census sources. The tenth-most common source is the Federal Reserve Bank of St. Louis, which is the only source that is not national.

$\rightarrow$ What are the different Associated Domains of the variables?

```{r}
dictionary %>%
  count(`Associated Domain`) %>%
  mutate(`Associated Domain` = fct_reorder(`Associated Domain`, n)) %>%
  ggplot(aes(x = `Associated Domain`, y = n/sum(n))) +
  geom_col() +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Associated Doman", y = "Percentage")
```
Most of the variables are demographic and housing variables. Some of them are also related to safety nets, economics, and outcomes.


A big part of this project will be figuring out what variables to include in the analysis.


### Data ethics


#### Data Science Ethics Checklist

[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)

**A. Problem Formulation**

 - [ ] **A.1 Well-Posed Problem**: Is it possible to answer our question with data? Is the problem well-posed?

**B. Data Collection**

 - [ ] **B.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?
 - [ ] **B.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?
 - [ ] **B.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?
 - [ ] **B.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?

**C. Data Storage**

 - [ ] **C.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?
 - [ ] **C.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?
 - [ ] **C.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?

**D. Analysis**

 - [ ] **D.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?
 - [ ] **D.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?
 - [ ] **D.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?
 - [ ] **D.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?
 - [ ] **D.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?

**E. Modeling**

 - [ ] **E.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?
 - [ ] **E.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?
 - [ ] **E.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?
 - [ ] **E.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?
 - [ ] **E.5 Communicate bias**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?

**F. Deployment**

 - [ ] **F.1 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?
 - [ ] **F.2 Roll back**: Is there a way to turn off or roll back the model in production if necessary?
 - [ ] **F.3 Concept drift**: Do we test and monitor for concept drift to ensure the model remains fair over time?
 - [ ] **F.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?

*Data Science Ethics Checklist generated with [deon](http://deon.drivendata.org).*


We will discuss these issues in class.


## Data Preparation



### Load the data 


The HUD data set is contained in the file `05b_analysis_file_update.csv`.


$\rightarrow$ Load the data set contained in the file `05b_analysis_file_update.csv` and name the data frame `df`.


<details>
  <summary>**Coding Hint**</summary>

Use `read_csv`, rather than `read.csv` because `read_csv` loads the data as a [tibble](https://r4ds.had.co.nz/data-import.html).

</details>
<br>

```{r}
df <- read_csv("05b_analysis_file_update.csv")
```


### Explore the contents of the data set


$\rightarrow$ Look at the first few rows of the data frame. 

```{r}
head(df)
```


Refer to the data dictionary and the [HUD report]((https://www.huduser.gov/portal/sites/default/files/pdf/Market-Predictors-of-Homelessness.pdf)) to understand what variables are present.


#### Explore the columns

What are the variables?
The data frame includes 332 different variables that are homeless rates at different times and places; possible predictors related to the local housing market, economy, community, and climate; and demographics/identifiers.

What variable(s) do we want to predict?
We want to predict the homeless rate.

What variables seem useful as predictors?
The housing market and community variables would probably be the most useful as predictors.

What predictor variables are redundant?
Some housing market variables seem like the same thing but from different sources. Many of the variables from the ACS 5 year estimates also have two each, depending on the year.

It will take a significant amount of work to understand the contents of the data set. We will discuss this in class.

#### Select a subset of columns

Below are suggested variables to keep in the analysis. You may include other variables that might be useful as predictors. Provide an explanation for why you kept the variables.

$\rightarrow$ Construct the list `variable_names` that we will keep for the analysis. 

```{r}
# Search through data dictionary to find other variables to include
variable_names <- c("year", "cocnumber",
  
  "pit_tot_hless_pit_hud", "pit_tot_shelt_pit_hud", "pit_tot_unshelt_pit_hud","dem_pop_pop_census",
  
  "fhfa_hpi_2009", "ln_hou_mkt_medrent_xt", "hou_mkt_utility_xt", "hou_mkt_burden_own_acs5yr_2017", "hou_mkt_burden_sev_rent_acs_2017", "hou_mkt_rentshare_acs5yr_2017", "hou_mkt_rentvacancy_xt", "hou_mkt_density_dummy", "hou_mkt_evict_count", "hou_mkt_ovrcrowd_acs5yr_2017", "major_city", "suburban",
           
           "econ_labor_unemp_rate_BLS", "econ_labor_incineq_acs5yr_2017", "econ_labor_pov_pop_census_share",
           
           "hou_pol_hudunit_psh_hud_share", "hou_pol_occhudunit_psh_hud", "hou_mkt_homeage1940_xt",
           
           "dem_soc_black_census", "dem_soc_hispanic_census", "dem_soc_asian_census", "dem_soc_pacific_census", "dem_pop_child_census", "dem_pop_senior_census", "dem_pop_female_census", "dem_pop_mig_census", "d_dem_pop_mig_census_share", "dem_soc_singadult_xt", "dem_soc_singparent_xt", "dem_soc_vet_xt", "dem_soc_ed_lessbach_xt", "dem_health_cost_dart",
           "dem_health_excesdrink_chr",
           
           "env_wea_avgtemp_noaa", "env_wea_avgtemp_summer_noaa", "env_wea_precip_noaa", "env_wea_precip_annual_noaa")

```

I kept the suggested variables from the course reader.

$\rightarrow$ Select this subset of variables from the full data set. Call the new data frame `df_small`.

```{r}
df_small <- df %>%
  select(all_of(variable_names))
```

$\rightarrow$ Examine the head of the new, smaller data frame.

```{r}
head(df_small)
```


$\rightarrow$ Create a new dictionary for this subset of variables and use `View` to examine the contents.

```{r}
dictionary_small <- dictionary %>%
  filter(Variable %in% variable_names)
```


How many variables of each Associated Domain are in the smaller data set?

$\rightarrow$ Make a bar graph of the counts of different `Associated Domain`. Your graph should have the following features:

1. Order the bars in descending order based on the count.
2. Orient the plot so that it is easy to read the labels.

```{r}
dictionary_small %>%
  count(`Associated Domain`) %>%
  mutate(`Associated Domain` = fct_reorder(`Associated Domain`, n)) %>%
  ggplot(aes(x = `Associated Domain`, y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Associated Domain", y = "Count")
```

Like the larger data set, most of the variables describe demographic and housing variables.

### Further exploration of basic properties


#### Check for a tidy data frame

In a tidy data set, each column is a variable or id and each row is an observation. It will take some work to determine whether the data set is tidy.

It was difficult to assess whether the full data frame was tidy because of the large number of columns with confusing names. We will do the analysis on the smaller data frame.




$\rightarrow$ How many observations are in the data set?

3008

### Data cleaning

#### Rename variables

I used the data dictionary to create more readable names for the minimal set of variables. You should add in new names for the additional variables you included in the data set.

The data frame with renamed columns is called `df_hud`.

```{r}
df_hud <- df_small %>% 
  rename(coc_number = dictionary_small$Variable[2],
    total_sheltered = dictionary_small$Variable[3],
total_unsheltered = dictionary_small$Variable[4],
total_homeless = dictionary_small$Variable[5],
total_population = dictionary_small$Variable[6],
total_female_population = dictionary_small$Variable[7],
total_population_0_19 = dictionary_small$Variable[8],
total_population_65_plus = dictionary_small$Variable[9],
total_black = dictionary_small$Variable[10],
total_asian = dictionary_small$Variable[11],
total_pacific_islander = dictionary_small$Variable[12],
total_latino_hispanic = dictionary_small$Variable[13],
house_price_index_2009 = dictionary_small$Variable[14],
rate_unemployment = dictionary_small$Variable[15],
net_migration = dictionary_small$Variable[16],
HUD_unit_occupancy_rate = dictionary_small$Variable[17],
number_eviction = dictionary_small$Variable[18],
percentage_excessive_drinking = dictionary_small$Variable[19],
medicare_reimbursements_per_enrollee = dictionary_small$Variable[20],
average_summer_temperature = dictionary_small$Variable[21],
total_annual_precipitation = dictionary_small$Variable[22],
average_Jan_temperature = dictionary_small$Variable[23],
total_Jan_precipitation = dictionary_small$Variable[24],
gini_coefficient_2016 = dictionary_small$Variable[25],
poverty_rate = dictionary_small$Variable[26],
share_renters_2016 = dictionary_small$Variable[27],
share_overcrowded_units_2016 = dictionary_small$Variable[28],
percentage_owners_cost_burden_2016 = dictionary_small$Variable[29],
percentage_renters_severe_cost_burden_2016 = dictionary_small$Variable[30],
share_HUD_units = dictionary_small$Variable[31],
high_housing_density = dictionary_small$Variable[32],
share_built_before_1940  = dictionary_small$Variable[33],
utility_costs  = dictionary_small$Variable[34],
rental_vacancy_rate = dictionary_small$Variable[35],
proportion_one_person_households  = dictionary_small$Variable[36],
share_under_18_with_single_parent = dictionary_small$Variable[37],
share_veteran_status = dictionary_small$Variable[38],
log_median_rent = dictionary_small$Variable[39],
migration_4_year_change  = dictionary_small$Variable[40],
share_no_bachelors = dictionary_small$Variable[41],
city_or_urban = dictionary_small$Variable[42],
suburban = dictionary_small$Variable[43]
)

```

$\rightarrow$ Examine the head of the new data frame with the updated names:

```{r}
head(df_hud)
```


$\rightarrow$ Display the names of the columns of the new data frame:


```{r}
names(df_hud)
```


### Identify and deal with missing values

$\rightarrow$ How many missing values are there in each column? Give the number of missing values and the percent of values in each column that are missing.

```{r}
skim(df_hud)
```
There are 14 missing values (0.4%) from total_homeless, total_sheltered, and total_unsheltered. There are 1504 missing values (50%) from log_median_rent, utility_costs, rental_vacancy_rate, share_built_before_1940, proportion_one_person_households, share_under_18_with_single_parent, share_veteran_status, and share_no_bachelors. There are 2632 missing values (87.5%) from migration_4_year_change and percentage_excessive_drinking. There are 2 missing values (0.06%) from average_summer_temperature.

We are interested in predicting the number of people experiencing homelessness. So, we will remove the rows where those numbers are missing.

$\rightarrow$ Remove the rows where `total_homeless` is `NA`.

```{r}
df_hud <- df_hud %>%
  filter(is.na(total_homeless) == FALSE)
```

There are some variables that are missing many of their values. 


$\rightarrow$ Produce scatter plots of the variables that are missing many values vs. time to see if data are missing from particular years.

```{r}
# Plot three variables that have over 1000 missing values
ggplot(data = df_hud, aes(x = year, y = log_median_rent)) +
  geom_point()
ggplot(data = df_hud, aes(x = year, y = percentage_excessive_drinking)) +
  geom_point()
ggplot(data = df_hud, aes(x = year, y = migration_4_year_change)) +
  geom_point()
```

Some variables only have data for 2017 (migration_4_year_change and percentage_excessive_drinking). These were the variables missing over 2000 values.

$\rightarrow$ Produce a data frame with data only from 2017. Call it `df_2017`.

```{r}
df_2017 <- df_hud %>%
  filter(year == 2017)
```


$\rightarrow$ Check for missing values in the 2017 data.

```{r}
skim(df_2017)
```
There are no missing values in the 2017 data set.

## Exploratory data analysis


We have two main goals when doing exploratory data analysis. The first is that we want to understand the data set more completely. The second goal is to explore relationships between the variables to help guide the modeling process to answer our specific question.


### Graphical summaries


####  Look at the distributions of the homeless counts


$\rightarrow$ Make a histogram of the total number of homeless in 2017.

```{r}
ggplot(data = df_2017, aes(x = total_homeless)) +
  geom_histogram(boundary = 0) +
  labs(x = "Total Homeless", y = "Count")
```

```{r}
# Set limits to better see the outlier values
ggplot(data = df_2017, aes(x = total_homeless)) +
  geom_histogram(boundary = 0) +
  coord_cartesian(ylim = c(0, 10)) +
  labs(x = "Total Homeless", y = "Count")
```

The data is mostly positively skewed, with lots of data points close to 0. There are also some outliers that are higher numbers, probably for larger cities.

#### Look at population distribution

```{r}
ggplot(data = df_2017, aes(x = total_population)) +
  geom_histogram(boundary = 0) +
  labs(x = "Total Population", y = "Count")
```

The total population histogram is also positively skewed. Most of the data points are near 0, with some outliers with higher populations. It matches the same shape as the total_homeless histogram.

$\rightarrow$ Make a histogram of the number of sheltered homeless in 2017.

```{r}
ggplot(data = df_2017, aes(x = total_sheltered)) +
  geom_histogram(boundary = 0) +
  labs(x = "Total Sheltered", y = "Count")
```
The graph is positively skewed, with one outlier close to 80000. It has the same shape as the total_homeless and total_population graphs.

$\rightarrow$ Make a histogram of the number of unsheltered homeless in 2017.

```{r}
ggplot(data = df_2017, aes(x = total_unsheltered)) +
  geom_histogram(boundary = 0) +
  labs(x = "Total Unsheltered", y = "Count")
```
The graph is also positively skewed, with an outlier past 40000. It matches all the other population graphs so far.


Converting the total counts to rates relative to population size should remove extreme outliers.


$\rightarrow$ Use the `mutate` function to create a new variable `rate_homeless` that is the total number of homeless per 10,000 people in the population and make a histogram of `rate_homeless`.

```{r}
df_2017 <- df_2017 %>%
  mutate(rate_homeless = total_homeless / (total_population / 10000))
ggplot(data = df_2017, aes(x = rate_homeless)) +
  geom_histogram(boundary = 0) +
  labs(x = "Homeless Rate per 10000 population", y = "Count")
```

The distribution is unimodal and positively skewed, though less skewed than the previous total homeless histogram. Most of the points continue to be clustered around the far left side of the graph. There are still outlier points on the right side of the graph.

$\rightarrow$ Compare boxplots of `rate_homeless` and `total_homeless` to visually assess whether outliers are less extreme in `rate_homeless` than in `total_homeless`.

```{r}
r <- ggplot(data = df_2017) +
  geom_boxplot(aes(y = rate_homeless))
t <- ggplot(data = df_2017) +
  geom_boxplot(aes(y = total_homeless))
grid.arrange(r, t, nrow = 1)
```
The outliers in the total_homeless box plot are more extreme. The quartiles of that box plot are harder to see, and it has two outlier points that are much higher than anything else on the plot. While the rate_homeless box plot also as a few outliers, they and the box can all be easily distinguished.

#### Data processing

We will add rates to the data frame. Following the HUD report, we will produce rates per 10,000 people.

$\rightarrow$ Use the `mutate` function to create new variables `rate_homeless`, `rate_sheltered`, and `rate_unsheltered` in the data frame `df_2017` that are the counts per 10,000 people in the population.

```{r}
df_2017 <- df_2017 %>%
  mutate(
    rate_sheltered = total_sheltered / (total_population / 10000),
    rate_unsheltered = total_unsheltered / (total_population / 10000))
```


We should note that the demographic variables (race, gender, age) are given as total counts. We will also convert these totals to percentages.


$\rightarrow$ Use the `mutate` function to create new demographics variables in the data frame `df_2017` that are percentages of the total population.

```{r}
# Convert all totals to percentages
df_2017 <- df_2017 %>%
  mutate(
    percent_black = total_black / total_population,
    percent_asian = total_asian / total_population,
    percent_latino_hispanic = total_latino_hispanic / total_population,
    percent_pacific_islander = total_pacific_islander / total_population,
    percent_0_19 = total_population_0_19 / total_population,
    percent_65_plus = total_population_65_plus / total_population,
    percent_female = total_female_population / total_population
  )
```


#### Basic summary

$\rightarrow$ How many people were experiencing homelessness in 2017? How many were sheltered and how many were unsheltered?

```{r}
df_2017 %>%
  select(total_homeless, total_sheltered, total_unsheltered) %>%
  colSums()
```

548312 people were homeless in 2017. 359669 of them were sheltered, and 188643 people were unsheltered.

$\rightarrow$ What are the minimum, maximum, mean, and median number of total, sheltered, and unsheltered homeless in 2017?

```{r}
df_2017 %>%
  select(total_homeless, total_sheltered, total_unsheltered) %>%
  summary()
```
Total homeless minimum: 10
Total homeless maximum: 76501
Total homeless mean: 1466.1
Total homeless median: 567.5

Total sheltered minimum: 5
Total sheltered maximum: 72565
Total sheltered mean: 961.7
Total sheltered median: 373

Total unsheltered minimum: 0
Total unsheltered maximum: 42828
Total unsheltered mean: 504.4
Total unsheltered median: 109.5

### Correlations between numerical variables


$\rightarrow$ Plot the correlation coefficients between all pairs of numerical variables using the `corrplot` function.

```{r}
df_2017 %>%
  select_if(is.numeric) %>%
  select(-"year") %>%
  cor(use = "pairwise.complete.obs") %>%
  corrplot(tl.cex = 0.3, type = "lower")
```
Many of the strongest correlations are between various demographic variables. Rate_homeless has strong correlations with a few housing market variables.


There are some variables that are highly correlated with the rate of homeless. Some are obvious, such as the rate of sheltered homeless. Other correlations show the existence of environmental variables, such as the share of overcrowded units, that are correlated with the rate of homeless.


Note the high correlation among subsets of the input variables. We will want to remove redundant variables or make a transformation of the input variables to deal with the correlation before constructing a model.


Next, we will find the variables with the highest correlation with `rate_homeless`. First, create the correlation matrix. Then examine the row with `rate_homeless` to find its correlation with the other variables.

```{r}

M <- df_2017 %>% 
  select_if(is.numeric) %>% 
  cor(use = "pairwise.complete.obs")


round(M["rate_homeless",],2)

```


Find the variables where the absolute value of the correlation with `rate_homeless` is greater than 0.3.

```{r}

M["rate_homeless",abs(M["rate_homeless",]) > 0.3 & is.na(M["rate_homeless",]) == FALSE] %>% 
  abs() %>%
  round(2) %>% 
  sort(decreasing = T)

```
There are 13 variables with correlation coefficients greater than 0.3.


$\rightarrow$ Make a pairs plot with a subset of the variables with the highest magnitude correlations. Select a small enough group so that you can see the panels. Do not include variables that you will not use as predictors (e.g. `rate_unsheltered`, any demographic totals)

```{r}
df_2017 %>% 
  select("rate_homeless", "share_renters_2016", "share_overcrowded_units_2016", "total_Jan_precipitation", "house_price_index_2009", "percentage_owners_cost_burden_2016", "log_median_rent", "utility_costs", "gini_coefficient_2016") %>% 
  filter("coc_number" != "CA-613") %>% 
  ggpairs(progress = FALSE, lower = list(continuous = "cor"), upper = list(continuous = "points"))
```


$\rightarrow$ Create additional plots to further understand the data set. Describe your findings.


```{r}
# Plot rate_homeless vs share_renters_2016
ggplot(data = df_2017, mapping = aes(x = share_renters_2016, y = rate_homeless)) +
  geom_point() +
  labs(x = "Share Renters 2016", y = "Homeless Rate per 100000 people")
```
Share_renters_2016 was the predictor with the highest correlation (0.48). The relationship between homeless rate and share_renters_2016 does not appear to be linear, since there are many points clustered near the bottom, and few higher up. An exponential relationship might fit the data better.

```{r}
ggplot(data = df_2017, mapping = aes(x = share_overcrowded_units_2016, y = rate_homeless)) +
  geom_point() +
  labs(x = "Share Overcrowded Units 2016", y = "Homeless Rate per 100000 people")
```
Share_overcrowded_units_2016 had the second highest coefficient (0.42). This relationship also does not appear linear. Many points are clustered on the bottom left of the graph.

```{r}
ggplot(data = df_2017, mapping = aes(x = total_Jan_precipitation, y = rate_homeless)) +
  geom_point() +
  labs(x = "Total January Precipitation", y = "Homeless Rate per 100000 people")
```
Total_Jan_precipitation had the third highest correlation (0.37). It also does not appear to have a linear relationship. It also has a larger spread of points, where there are many points with high precipitation.

## Model


We will multiple approaches to construct models that predict `rate_homeless`:

1. Use statistical significance to create a multiple linear regression model.
2. Best subset selection for a multiple linear regression model.
3. Lasso
4. Ridge regression
5. XGBoost

To compare the different approaches, we will use a training and testing split of the data set.

### Set up the data set for training and testing


#### Remove some variables

There are several variables that we do not want to include as predictors. We want to remove demographic totals, the year, the CoC number, and the other homeless rates that we are not predicting. You may have additional variables to remove to create the data set that contains only the response variable and the predictors that you want to use.

```{r}
# Remove non predictor variables
variable_remove = c("total_homeless", "total_sheltered", "total_unsheltered", "total_black", "total_latino_hispanic", "total_asian", "total_pacific_islander", "total_population_0_19", "total_population_65_plus", "total_female_population", "year", "coc_number", "total_population", "rate_unsheltered", "rate_sheltered")

df_2017 <- df_2017 %>% 
  select(-all_of(variable_remove))

# Display remaining variables
names(df_2017)

```

These variables are removed because they do not work as predictor variables for the homelessness rate. Instead, many of them are directly related, such as the total_homeless, rate_unsheltered, and rate_sheltered. Because of the different population sizes in various cities, we want to use rates instead of total numbers, which is why all of those variables are removed from the data set.

$\rightarrow$ How many input variables remain in the data set that you can use as predictors?

```{r}
skim(df_2017)
```

There are 37 variables that can be used as predictors, and 374 observations. This can make it difficult to pick the best model, since it would be easier to overfit the data. The model could be accurate to the small data set, but not be able to predict any new data.

#### Get train and test splits


We will split the data into training and testing sets, with 85% of the data kept for training.   

```{r}
# Set seed
set.seed(20)

# Split data (85% training, 15% test)
split <- initial_split(df_2017, prop = 0.85, strata = rate_homeless)

# Get train/test split
df_train <- training(split)
df_test <- testing(split)

```


$\rightarrow$ Check the sizes of the training and testing splits

```{r}
skim_without_charts(df_train)
skim_without_charts(df_test)
```

There are 316 data entries in the training data and 58 in the testing data.


### Full regression model


#### Fit the model on the training data

$\rightarrow$ Use the training data to fit a multiple linear regression model to predict `rate_homeless` using all possible predictor variables.

```{r}
fit_all <- lm(rate_homeless ~ ., data = df_train)
```


$\rightarrow$ Examine the summary of the fit. Which predictors have statistically significant coefficients? Do the signs of the coefficients make sense?

```{r}
summary(fit_all)
```
At the 0.05 level, share_overcrowded_units_2016 is significant. At the 0.01 level, share_HUD_units and proportion_one_person_households are also significant. At the 0.001 level, total_Jan_precipitation, proportion_one_person_households, migration_4_year_change, and log_median_rent are also significant. Though not a predictor, the demographic variable percent_black was also marked as significant at the 0.01 level.


We should also do a qualitative assessment of the fit to the training data.

$\rightarrow$ Plot the residuals to look for systematic patterns of residuals that might suggest a new model.

```{r}
plot(fit_all, 1)
title(main = "Residuals vs Fitted")
```

The residual model has a systemic pattern where many more data points are clustered on the left. The linear regression with all variables is not the best model to fit the data.

#### Assess the model on the testing data

$\rightarrow$ Use the model to predict the homeless rate in the testing data.

```{r}
lm_predict <- predict(fit_all, df_test)
```


$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}
plot(df_test$rate_homeless, lm_predict, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
title(main = "Actual vs Predicted Homeless Rates")
abline(0, 1)
```

<br>

The graph shows that the linear model with all the variables predicts the homeless rate fairly well for lower rates, though the model tends to overestimate the rates. At high actual homeless rates, there is one point that is predicted to be much lower.

$\rightarrow$ Compute the RMSE

```{r}
rmse <- (df_test$rate_homeless - lm_predict)^2 %>%
  mean() %>%
  sqrt()
rmse
```
The RMSE is 11.46.

$\rightarrow$ Repeat the regression analysis, but using a different random seed before the train-test split. Does the train-test split affect the significance of the coefficients and their values?

```{r}
# Set seed to 10
set.seed(10)

# Split data
split_10 <- initial_split(df_2017, prop = 0.85, strata = rate_homeless)

# Get train and test split
df_train_10 <- training(split_10)
df_test_10 <- testing(split_10)

# Fit multiple linear regression
fit_10 <- lm(rate_homeless ~ ., data = df_train_10)
summary(fit_10)

# Graph residuals
plot(fit_10, 1)

# Predict for testing data, 
predict_10 <- predict(fit_10, df_test_10)
plot(df_test_10$rate_homeless, predict_10, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
abline(0, 1)
```
<br>
The residual graph is similar to the residual graph from the first seed. The points continue to be systemically structured in a curve. The actual vs predicted graph is also similar, except for the points with high actual homeless rates. The coefficients are different, and give different significant predictor variables. When the seed was 20, migration_4_year_change, log_median_rent, and total_Jan_precipitation had the smallest p-values. When the seed was 10, total_Jan_precipitation and proportion_one_person_households had the smallest p-values.

### Subset selection

The full model contains too many predictors, so we will use subset selection on the training data to find a smaller number of predictors.


$\rightarrow$ Get the number of variables in the data set that will be used as predictors. This will be used to set the subset size.

```{r}
n <- ncol(df_train) - 1
n
```
There are 37 variables in the data set that will be used as predictors.

$\rightarrow$ Do best subset selection on the training data. Set the maximum number of variables to equal the number of predictor variables in the data set.

```{r}
regfit_all <- regsubsets(rate_homeless ~ ., data = df_train, nvmax = n)
```



$\rightarrow$ Get the summary. We will use this after performing cross validation to determine the best model.

```{r}
regfit_summary <- summary(regfit_all)
```


#### Cross validation

$\rightarrow$ Use 10-fold cross validation on the training data to determine the best subset of predictors in the model.

```{r}
# Set number of observations (rows)
row <- nrow(df_train)
 
# Set number of folds
num_fold <- 10
 
# Set seed
set.seed(1)
 

folds <- sample(1:num_fold, row, replace = TRUE)
error <- matrix(NA, num_fold, n, dimnames = list(NULL, paste(1:n)))
  
for (i in 1:num_fold) {
  reg_fit_best <- regsubsets(rate_homeless ~ ., data = df_train[folds != i,], nvmax = n)
  test_mat <- model.matrix(rate_homeless ~ ., data = df_train[folds == i,])
  for (j in 1:n) {
    c <- coef(reg_fit_best, id = j)
    p <- test_mat[,names(c)] %*% c
    error[i, j] <- mean((df_train$rate_homeless[folds == i] - p)^2)
  }
}
 
error_mean <- apply(error, 2, mean)
```

$\rightarrow$ Plot the assessment measures vs. the number of predictors

```{r}
# Set up 2x2 graphs
par(mfrow = c(2, 2))

# Find number of variables for minimum cp
# Sometimes this changes?
ind_cp <- which.min(regfit_summary$cp)
# Plot graph, title with number of variables
plot(regfit_summary$cp, type = "b", xlab = "Number of Variables", ylab = "Cp", main = toString(ind_cp))
# Color minimum point red
points(ind_cp, regfit_summary$cp[ind_cp], col = "red", pch = 20)

# Find number of variables for minimum bic
ind_bic = which.min(regfit_summary$bic)
# Plot graph, title with number of variables
plot(regfit_summary$bic, type = "b", xlab = "Number of Variables", ylab = "BIC", main = toString(ind_bic))
# Color minimum point red
points(ind_bic, regfit_summary$bic[ind_bic], col = "red", pch = 20)

# Find number of variables for maximum adjusted r squared
ind_adjr2 = which.max(regfit_summary$adjr2)
# Plot graph, title with number of variables
plot(regfit_summary$adjr2, type = "b", xlab = "Number of Variables", ylab = "Adjusted R Squared", main = toString(ind_adjr2))
# Color maximum point red
points(ind_adjr2, regfit_summary$adjr2[ind_adjr2], col = "red", pch = 20)

# Find number of variables to minimize mean error
ind_cv <- which.min(error_mean)
# Plot graph
plot(error_mean, type = "b", xlab = "Number of Variables", ylab = "Cross Validation", main = toString(ind_cv))
# Color minimum point red
points(ind_cv, error_mean[ind_cv], col = "red", pch = 20)
```
Each measure has a different number of variables. Minimizing the Cp has 13 variables, minimizing the BIC has 10 variables, maximizing the adjusted R squared has 20 variables, and minimizing the mean error has 9 variables.


$\rightarrow$ What are the coefficients of the best model according to different measures?

```{r}
# Cp coefficients
coef(regfit_all, ind_cp) %>%
  round(2) %>%
  data.frame()
```


```{r}
# BIC coefficients
coef(regfit_all, ind_bic) %>%
  round(2) %>%
  data.frame()
```
The coefficients of the BIC model are close to those of the full linear model. The coefficient of share_HUD_units is the same in both cases: 2.04.

```{r}
# Adjusted r squared coefficients
coef(regfit_all, ind_adjr2) %>%
  round(2) %>%
  data.frame()
```
The coefficients of the adjusted r squared model are similar to those of the full linear model. They all have the same signs and similar magnitudes.

```{r}
# Cv coefficients
coef(regfit_all, ind_cv) %>%
  round(2) %>%
  data.frame()
```
Most of the coefficients are of the same sign and magnitude as the full model. The biggest difference is the demographic variable percent_black. In the full model, it had a coefficient of -39.48. Here, the coefficient is -20.89, which is around half. None of the predictor values had drastic differences.

#### Assess the performance of the model on the testing data

Use the model to predict the homeless rate in the testing data. We will use the best BIC model as an example, but you should compare the performance of all models.

##### Cp Model Analysis

```{r}
# Select best variable subset from Cp analysis
df_cp <- df_train %>%
  select(all_of(names(coef(regfit_all, ind_cp))[2:length(coef(regfit_all, ind_cp))]), "rate_homeless")

# Fit linear regression model from subset
lm_cp <- lm(rate_homeless ~ ., data = df_cp)
summary(lm_cp)

# Predict homeless rate from model
pred_cp <- predict(lm_cp, df_test)

# Plot actual vs predicted homeless rate
plot(df_test$rate_homeless, pred_cp, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
# Add x = y line
abline(0, 1)

# Calculate RMSE
rmse_cp <- (df_test$rate_homeless - pred_cp)^2 %>%
  mean() %>%
  sqrt()
rmse_cp
```

Not all of the variables are significant. At the 0.05 level, high_housing_density, share_overcrowded_units_2016, suburban, share_HUD_units, migration_4_year_change, proportion_one_person_households, share_no_bachelors, percentage_excessive_drinking, and total_Jan_precipitation are significant. The model also includes demographic variables. In the actual vs predicted homeless rate model, the predicted values are higher than the actual values. The RMSE is 11.85. It's close to the RMSE of the full model (11.46).


##### BIC Model Analysis

```{r}
# Select subset of variables from best subset selection
df_bic <- df_train %>%
  select(all_of(names(coef(regfit_all, ind_bic))[2:length(coef(regfit_all, ind_bic))]), "rate_homeless")

# Fit model from subset
lm_bic <- lm(rate_homeless ~ ., data = df_bic)
summary(lm_bic)

# Predict rate_homeless from model
pred_bic <- predict(lm_bic, df_test)

# Plot actual vs predicted omeless rate
plot(df_test$rate_homeless, pred_bic, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
# Add x = y line
abline(0, 1)

# Calculate RMSE
rmse_bic <- (df_test$rate_homeless - pred_bic)^2 %>%
  mean() %>%
  sqrt()
rmse_bic
```

All the variables included in the model are significant at the 0.01 level. In the actual vs predicted homeless rate graph, most of the points are clustered about the x = y line, so the model predicts higher values for the homeless rate than they actually are. The RMSE is 9.82, which is lower than the RMSE for the full model. The BIC model is a better fit than the full model.

##### Adjusted R Squared Model Analysis
```{r}
# Select best variables from adjusted R square analysis
df_adjr2 <- df_train %>%
  select(all_of(names(coef(regfit_all, ind_adjr2))[2:length(coef(regfit_all, ind_adjr2))]), "rate_homeless")

# Fit model with data subset
lm_adjr2 <- lm(rate_homeless ~ ., data = df_adjr2)
summary(lm_adjr2)

# Predict homeless rate
pred_adjr2 <- predict(lm_adjr2, df_test)

# Plot measured vs predicted values for rate_homeless
plot(df_test$rate_homeless, pred_adjr2, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
# Add x = y line
abline(0, 1) 

# Calculate RMSE
rmse_adjr2 <- (df_test$rate_homeless - pred_adjr2)^2 %>%
  mean() %>%
  sqrt()
rmse_adjr2
```
Not all variables are significant. At the 0.05 level, share_overcrowded_units_2016, suburban, share_HUD_units, migration_4_year_change, proportion_one_person_households, and total_Jan_precipitation are significant. Comparing the actual and predicted values shows that the homeless rate is predicted higher than it actually is.  The RMSE is 11.25. It is similar to the RMSE for the full model.

##### CV Model Analysis

```{r}
# Select best variables from CV analysis
df_cv <- df_train %>%
  select(all_of(names(coef(regfit_all, ind_cv))[2:length(coef(regfit_all, ind_cv))]), "rate_homeless")

# Fit model with data subset
lm_cv <- lm(rate_homeless ~ ., data = df_cv)
summary(lm_cv)

# Predict homeless rate
pred_cv <- predict(lm_cv, df_test)

# Plot measured vs predicted values for rate_homeless
plot(df_test$rate_homeless, pred_cv, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
title(main = "Actual vs Predicted Homeless Rate")
# Add x = y line
abline(0, 1) 

# Calculate RMSE
rmse_cv <- (df_test$rate_homeless - pred_cv)^2 %>%
  mean() %>%
  sqrt()
rmse_cv
```
All the included variables are significant. The actual vs predicted graph is even for lower homeless rates, and differs for higher rates. The RMSE is 9.017.

### Lasso


The lasso is another approach to producing a linear regression model with a subset of the available predictors. The lasso works by finding coefficients $\boldsymbol{\beta}$ that minimize the cost function:

$$C(\boldsymbol{\beta}) = \sum_{i=1}^n(y_i - \beta_{0} - \sum_{j=1}^p\beta_{j}x_{ij})^2 + \lambda \sum_{j=1}^p|\beta_{j}| = \text{RSS} + \lambda \sum_{j=1}^p|\beta_{j}|$$

where $\lambda \geq 0$ is a tuning parameter (or hyperparameter).

$\rightarrow$ Prepare the data by creating a model matrix `x_train` from the training data. Create the model matrix using `model.matrix` so that it includes the training data for all predictors, but does not include a column for the intercept. Also create the training response data `y_train`.

```{r}
# Create model matrix, remove intercept
x_train <- model.matrix(rate_homeless ~ ., df_train)[,-1]
# Create training response
y_train <- df_train$rate_homeless
```


$\rightarrow$ Use cross-validation to find the best hyperparameter $\lambda$

```{r}
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)
```


$\rightarrow$ Show error as a function of the hyperparameter $\lambda$ and its best value.

```{r}
plot(lasso_cv)
lasso_best_lam <- lasso_cv$lambda.1se
lasso_best_lam
```
From the graph, the minimum lambda is a model with 20-23 variables, and the one standard error lambda is a model with 7 variables. I chose to use the one standard error as my lambda: 2.59.

$\rightarrow$ Fit the lasso with the best $\lambda$ using the function `glmnet`.

```{r}
lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = lasso_best_lam)
```


$\rightarrow$ Examine the coefficients. Which variables have non-zero coefficients?

```{r}
coef(lasso_mod)
```

The predictor variables with non-zero coefficients are utility_costs, share_renters_2016, share_overcrowded_units_2016, and total_Jan_precipitation. Demographic variables percent_pacific_island and percent_0_19 also have non-zero coefficients.

#### Look at prediction error


$\rightarrow$ Use the model to predict the homeless rate in the testing data.

```{r}
x_test <- model.matrix(rate_homeless ~ ., df_test)[,-1]
lasso_predict <- predict(lasso_mod, s = lasso_best_lam, newx = x_test)
```


$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}
plot(df_test$rate_homeless, lasso_predict, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
title(main = "Actual vs Predicted Homeless Rates")
abline(0, 1)
```
<br>
The predictions are fairly accurate for lower homeless rates. The model tends to overestimate the rate for lower actual homeless rates. At higher actual homeless rates, the model underestimates the rate.

$\rightarrow$ Compute the RMSE. How does it compare to the other models?

```{r}
lasso_rmse <- (df_test$rate_homeless - lasso_predict)^2 %>%
  mean() %>%
  sqrt()
lasso_rmse
```
The RMSE is 9.06.


### Ridge regression


Ridge regression is another approach to model building 

Ridge regression works by finding coefficients $\boldsymbol{\beta}$ that minimize the cost function:

$$C(\boldsymbol{\beta}) = \sum_{i=1}^n(y_i - \beta_{0} - \sum_{j=1}^p\beta_{j}x_{ij})^2 + \lambda \sum_{j=1}^p\beta_{j}^2 = \text{RSS} + \lambda \sum_{j=1}^p\beta_{j}^2$$

In contrast to the lasso, ridge regression will not reduce the number of non-zero coefficients in the model. Ridge regression will shrink the coefficients, which helps to prevent overfitting of the training data.


The fitting procedure for the ridge regression model mirrors the lasso approach, only changing the parameter $\alpha$ to 0 in the `cv.glmnet` and `glmnet` functions.


$\rightarrow$ Fit and assess a ridge regression model. How does it compare to the other models?

```{r}
# Perform ridge cross validation
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0)

# Find best lambda
ridge_best_lam <- ridge_cv$lambda.1se
ridge_best_lam
```

The best lambda value is 68.74. I used the one standard error lambda for the ridge as well.


```{r}
# Fit model
ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = ridge_best_lam)

# Predict values from model
ridge_predict <- predict(ridge_mod, s = ridge_best_lam, newx = x_test)

# Plot actual vs predicted values
plot(df_test$rate_homeless, ridge_predict, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
title(main = "Actual vs Predicted Homeless Rate")
abline(0, 1)
```
<br>

The model overestimates the homeless rates for lower actual homeless rates, and underestimates it for higher actual homeless rates. A few of the points from the 10-20 range lie directly on the x = y line.

```{r}
# Calculate RMSE
ridge_rmse <- (df_test$rate_homeless - ridge_predict)^2 %>%
  mean() %>%
  sqrt()
ridge_rmse
```

The RMSE is 9.16. It is slightly greater, but close to the RMSE from the lasso model.


### XGBoost

XGBoost is short for eXtreme Gradient Boosting.

We are going to use the `tidymodels` package to fit the XGBoost model.


#### Set up the model

The model will be a boosted tree model, so we start by specifying the features of a `boost_tree` model. The`boost_tree` creates a specification of a model, but does not fit the model.

```{r}

xgb_spec <- boost_tree(
  mode = "regression",  #We are solving a regression problem
  trees = 1000, 
  tree_depth = tune(),  # tune() says that we will specify this parameter later
  min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune(),                         
  ) %>% 
  set_engine("xgboost", objective = "reg:squarederror") ## We will use xgboost to fit the model and try to minimize the squared error

xgb_spec

```

Create a workflow that specifies the model formula and the model type. We are still setting up the model; this does not fit the model.

```{r}

xgb_wf <- workflow() %>%
  add_formula(rate_homeless ~ .) %>%
  add_model(xgb_spec)

xgb_wf
```


#### Fit the model

We need to fit all of the parameters that we specified as `tune()`. We will specify the parameter grid using the functions `grid_latin_hypercube`:

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 30  #Create 30 sets of the 6 parameters
)
```


Create folds for cross-validation. 

```{r}
folds <- vfold_cv(df_train)
```


Do the parameter fitting. This will take some time.

```{r}

xgb_res <- tune_grid(
  xgb_wf,              #The workflow
  resamples = folds,   #The training data split into folds
  grid = xgb_grid,     #The grid of parameters to fit
  control = control_grid(save_pred = TRUE)
)

xgb_res
```

Set up the final workflow with the best model parameters.

```{r}

#Get the best model, according to RMSE
best_rmse <- select_best(xgb_res, "rmse")

#Update the workflow with the best parameters
final_xgb <- finalize_workflow(
  xgb_wf,
  best_rmse
)

final_xgb
```


#### Prediction


Fit the final model to the training data and predict the test data.
```{r}

final_res <- last_fit(final_xgb, split)

```


Show the RMSE. Compare the result to the test RMSE for the other models.

```{r}
collect_metrics(final_res)
```
The RMSE is 9.15. It is similar to the RMSE from the lasso and ridge models.

Plot the ACT scores and the prediction

```{r}

plot(final_res$.predictions[[1]]$rate_homeless,final_res$.predictions[[1]]$.pred, xlab = "Actual Homeless rate", ylab = "Predicted homeless rate")
title(main = "Actual vs Predicted Homeless Rate")
abline(0, 1)

```
<br>
The model is fairly accurate at predicting most of the actual homeless rates. The points are evenly divided around the x = y line. Only two points at higher homeless rates are off: one much higher than the line, and one much lower.


#### Relative importance of predictors

Look at which predictors are most important in the model

```{r}

final_xgb %>%
  fit(data = df_train) %>%
  extract_fit_parsnip() %>%
  vip(geom = "col")

```
The most important variable is share_renters_2016, fllowed by total_Jan_precipitation and share_HUD_units.


### Compare models

You used several methods to construct a model

1. Use statistical significance to create a multiple linear regression model.
2. Best subset selection for a multiple linear regression model.
3. Lasso
4. Ridge regression
5. XGBoost

Compare the performance of the models. 

The full linear regression model has three most significant variables: log_median_rent, migration_4_year_change, and total_Jan_precipitation. The plot of the residuals has a systemic structure, where many of the points are clustered on the left in an almost parabolic shape. In the plot of actual vs predicted homeless rate, the lower rates are pretty evenly distributed around the x = y line. At higher actual homeless rates, the predictions are more off. The RMSE is 11.46.

The different linear regression subset selections have slightly different values for the RMSE, and similar actual vs predicted homeless rate graphs. The Cp model includes 15 variables, though not all of them are significant. The RMSE is 11.71, which is slightly higher than the RMSE of the full linear model. The BIC model has an RMSE of 9.82, and includes 10 significant variables. The adjusted r squared model has an RMSE of 11.25 and 20 variables. Not all of the variables in this model are significant. The cross validation model has 3 significant variables and an RMSE of 9.02, which the lowest RMSE from all the linear models. The three variables included in the CV model are share_overcrowded_units_2016, proportion_one_person_households, and total_Jan_precipitation. All three of these variables are included and significant in all the other subset selections.

In the lasso model, 6 of the variables have non-zero coefficients. The predicted vs actual homeless rate plot looks similar to those from the all the linear regression models. The predicted values are close for lower homeless rates, and off for higher ones. The RMSE is 9.06. It's lower than the RMSE from the full linear regression model, and close to the cross validation subset.

In the ridge model, the RMSE is 9.16. Its predicted vs actual homeless rate plot looks similar to those of the linear and lasso regression models, though it has some points on the x = y line, unlike those models.

The tree model has an RMSE of 9.15, similar to the CV subset, lasso, and ridge models. The most important variables in this model are share_renters_2016, total_Jan_precipitation, share_HUD_units, proportion_one_person_households, and rental_vacancy_rate.

Overall, the RMSEs of the different models range from 8.80 to 11.46. Most of the models have RMSE values around 9, including the BIC subset, lasso, ridge, and tree models. There were two variables included in every model: total_Jan_precipitation and share_overcrowded_units_2016. Total_Jan_precipitation was significant in every linear model subset, and the second most important variable to the tree model. Share_overcrowded_units_2016 was also significant in all the linear models, and was the seventh most important predictor in the tree model. Other variables appeared only in a few models. Share_renters_2016 was the most important variable in the tree model, and was also included in the lasso model, but not in any of the linear model selections. In all the linear models, share_HUD_units, log_median_rent, proportion_one_person_households, and migration_4_year_change were significant. Share_HUD_units and proportion_one_person_households were also included in the tree model, but not the lasso model. The lasso model and tree model included utility_costs, which was not significant in any of the linear subset selections.

## Additional step

In addition to completing the above analyses, you should perform any revisions to the models that you think might improve their performance. Consult Canvas for further directions.

### Question

I chose to look at how the location may have an effect on the most significant variables. The original large data set has a variable for the four census regions (northeast, midwest, south, west), labelled from 1 to 4 in the same order as the US Census. I did a lasso regression analysis with the combination of the census region and the four predictor variables with non-zero coefficients from my original analysis. I chose to do a lasso analysis because it had the lowest RMSE from my initial analysis, though that has changed.

### Data Re-organization

```{r}
# Select variables from original lasso analysis and add census_region
var_region <- c("year",
  
  "pit_tot_hless_pit_hud", "dem_pop_pop_census",
  
  "hou_mkt_utility_xt", "hou_mkt_rentshare_acs5yr_2017", "hou_mkt_ovrcrowd_acs5yr_2017",
          
  "env_wea_precip_noaa",
  
  "census_region"
  )

# Filter original data set to data from variable list and 2017
df_region <- df %>%
  select(all_of(var_region)) %>%
  filter(year == 2017)
```

```{r}
# Rename variables to match rest of analysis
df_region <- df_region %>% 
   rename(
total_homeless = "pit_tot_hless_pit_hud",
total_population = "dem_pop_pop_census",
total_Jan_precipitation = "env_wea_precip_noaa",
share_renters_2016 = "hou_mkt_rentshare_acs5yr_2017",
share_overcrowded_units_2016 = "hou_mkt_ovrcrowd_acs5yr_2017",
utility_costs  = "hou_mkt_utility_xt",
)

# Get rid of entries with missing total homeless entries
df_region <- df_region %>%
  filter(is.na(total_homeless) == FALSE)
```

```{r}
# Convert from total to rate, and change census_region to factor
df_region <- df_region %>%
  mutate(rate_homeless = total_homeless / (total_population / 10000)) %>%
  mutate(census_region = as.factor(census_region))
```

```{r}
set.seed(20)

# Split data again
split_region <- initial_split(df_region, prop = 0.85, strata = rate_homeless)

train_region <- training(split_region)
test_region <- testing(split_region)
```



### Analysis
```{r}
# Set up model matrix
# Use combination of predictor variables and census region
x_train_region <- model.matrix(rate_homeless ~ utility_costs*census_region + share_renters_2016*census_region + share_overcrowded_units_2016*census_region + total_Jan_precipitation*census_region, train_region)[,-1]
y_train_region <- train_region$rate_homeless
```

```{r}
lasso_region_cv <- cv.glmnet(x_train_region, y_train_region, alpha = 1)
```

```{r}
plot(lasso_region_cv)
```
<br>
The minimum lambda has seven variables and the one standard error lambda has three variables.

```{r}
lasso_region_best_lam <- lasso_region_cv$lambda.1se
lasso_region_best_lam
```
The best lambda value (using one standard error) is 4.69.
```{r}
lasso_region_mod <- glmnet(x_train_region, y_train_region, alpha = 1, lambda = lasso_region_best_lam)

coef(lasso_region_mod)
```
The variables with non-zero coefficients are share_renters_2016, the combination of share_renters_2016 and census_region4, and the combination of total_Jan_precipitation and census_region4.

```{r}
# Predict for test data
x_test_region <- model.matrix(rate_homeless ~ utility_costs*census_region + share_renters_2016*census_region + share_overcrowded_units_2016*census_region + total_Jan_precipitation*census_region, test_region)[,-1]
lasso_region_predict <- predict(lasso_region_mod, s = lasso_region_best_lam, newx = x_test_region)
```

```{r}
plot(test_region$rate_homeless, lasso_region_predict, xlab = "Actual Homeless Rate", ylab = "Predicted Homeless Rate")
title(main = "Actual vs Predicted Homeless Rate")
abline(0, 1)
```
<br>

The model is consistent with the initial lasso analysis. It overestimates homeless rates for lower rates, and underestimates it at higher rates.

```{r}
lasso_region_rmse <- (test_region$rate_homeless - lasso_region_predict)^2 %>%
  mean() %>%
  sqrt()
lasso_region_rmse
```

The RMSE is 9.35. It is similar to most of the RMSE values from the other models.

### Discussion

The only census region that had a non-zero coefficient in combination with another variable was census region 4, which is west. This region includes the entire west half of the continental US, Alaska, and Hawaii. Of the four variables, two of them (share_renters_2016 and total_Jan_precipitation) were significant with region 4. Total_Jan_precipitation did not have a coefficient on its own.

From these results, share_renters_2016 works as a predictor for homeless rates generally, and share_renters_2016 and total_Jan_precipitation can predict homeless rates in the western US.

## Conclusion

After completing your analyses, you will make your conclusions and communicate your results. Consult Canvas for further directions.

Based on the RMSE values, the best model for predicting homeless rates is cross-validation as the measure for best subset selection from a linear model, because it had the lowest RMSE of 8.79. The most important variables for predicting homeless rates are total January precipitation and share of overcrowded housing units. Both of these variables were included and significant in every model used. 

One limitation of the analysis has to do with the data. The data set has a small number of data points, but includes many variables for possible predictors. This makes it easier to possibly overfit the data, and fit the noise instead of the more general trend. Since we also split the data into training and testing sets, that also lessened the number of data points with which to perform the analyses. The predictor variables could also be related to each other, which would not be very apparent in an analysis between one predictor and homeless rate. In my additional step, I explored one possible relationship between region and homeless rate, using only the predictors generated by my lasso analysis. From this analysis, only census region 4 (west) had any affect on the homeless rate. It stands to reason that homeless rate in the west can be accurately predicted by the share of renters and total January precipitation. These results are consistent with the results of the larger analysis, since total January precipitation seems to be a very important predictor.

Another limitation of the analysis could come from the initial data set. While much of the data comes from the US Census and is reasonably reliable, the HUD presentation notes that some of the data were estimates, even for variables such as homeless rate. 